{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# Credit Card Fraud Detection\n",
                "## Complete Analysis and Model Training\n",
                "\n",
                "This notebook provides a comprehensive analysis of credit card fraud detection using machine learning.\n",
                "\n",
                "### Table of Contents:\n",
                "1. [Setup & Dependencies](#setup)\n",
                "2. [Data Extraction](#extraction)\n",
                "3. [Exploratory Data Analysis](#eda)\n",
                "4. [Model Training](#training)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "setup-header",
            "metadata": {},
            "source": [
                "## 1. Setup & Dependencies Installation <a id='setup'></a>\n",
                "\n",
                "Installing all required libraries for data analysis and machine learning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "install-deps",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q --upgrade pip\n",
                "!pip install -q pandas numpy matplotlib seaborn scikit-learn imbalanced-learn xgboost joblib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "import-libs",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import all necessary libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import zipfile\n",
                "import os\n",
                "\n",
                "# Machine Learning Libraries\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (\n",
                "    accuracy_score,\n",
                "    confusion_matrix,\n",
                "    classification_report,\n",
                "    precision_score,\n",
                "    recall_score,\n",
                "    f1_score,\n",
                "    roc_auc_score,\n",
                "    roc_curve\n",
                ")\n",
                "from imblearn.over_sampling import SMOTE\n",
                "import xgboost as xgb\n",
                "import joblib\n",
                "\n",
                "# Settings\n",
                "warnings.filterwarnings('ignore')\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "print('All libraries imported successfully!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "extraction-header",
            "metadata": {},
            "source": [
                "## 2. Data Extraction <a id='extraction'></a>\n",
                "\n",
                "Download and extract the credit card fraud dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "download-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create data directory if it doesn't exist\n",
                "data_dir = '../data'\n",
                "if not os.path.exists(data_dir):\n",
                "    os.makedirs(data_dir)\n",
                "    print(f'Created directory: {data_dir}')\n",
                "\n",
                "# Download the dataset\n",
                "dataset_url = 'https://www.kaggle.com/api/v1/datasets/download/mlg-ulb/creditcardfraud'\n",
                "!curl -L -o {data_dir}/creditcardfraud.zip {dataset_url}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "extract-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract the dataset\n",
                "zip_path = f'{data_dir}/creditcardfraud.zip'\n",
                "extract_path = data_dir\n",
                "\n",
                "if os.path.exists(zip_path):\n",
                "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
                "        zip_ref.extractall(extract_path)\n",
                "    print('Dataset extracted successfully!')\n",
                "    print(f'Files in {data_dir}:')\n",
                "    for file in os.listdir(data_dir):\n",
                "        print(f'  - {file}')\n",
                "else:\n",
                "    print('Error: Zip file not found!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "df = pd.read_csv(f'{data_dir}/creditcard.csv')\n",
                "print('Dataset loaded successfully!')\n",
                "print(f'Dataset shape: {df.shape}')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "splitting-header",
            "metadata": {},
            "source": [
                "## 2.1 Data Splitting for Simulation and Training <a id='splitting'></a>\n",
                "\n",
                "Splitting the dataset into Training, Testing, and Simulation sets.\n",
                "Simulation sets are reserved for fog node simulation and contain fraudulent transactions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "split-data-logic",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate fraud and normal transactions\n",
                "fraud_df = df[df['Class'] == 1].copy()\n",
                "normal_df = df[df['Class'] == 0].copy()\n",
                "\n",
                "print(f'Total Fraud Cases: {len(fraud_df)}')\n",
                "print(f'Total Normal Cases: {len(normal_df)}')\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# --- Simulation Node 1 ---\n",
                "sim_node_1_fraud = fraud_df.sample(n=20, random_state=42)\n",
                "sim_node_1_normal = normal_df.sample(n=2000, random_state=42)\n",
                "simulation_node_1 = pd.concat([sim_node_1_fraud, sim_node_1_normal]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
                "\n",
                "print(f\"Node 1 Fraud: {simulation_node_1['Class'].sum()}\")\n",
                "print(f\"Node 1 Normal: {len(simulation_node_1) - simulation_node_1['Class'].sum()}\")\n",
                "\n",
                "# Save node1 simulation data\n",
                "simulation_node_1.to_csv(f'{data_dir}/simulation_node_1.csv', index=False)\n",
                "print(f'Simulation Node 1 saved: {simulation_node_1.shape}')\n",
                "\n",
                "# Remove sampled data\n",
                "fraud_df = fraud_df.drop(sim_node_1_fraud.index)\n",
                "normal_df = normal_df.drop(sim_node_1_normal.index)\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# --- Simulation Node 2 ---\n",
                "sim_node_2_fraud = fraud_df.sample(n=20, random_state=43)\n",
                "sim_node_2_normal = normal_df.sample(n=2000, random_state=43)\n",
                "simulation_node_2 = pd.concat([sim_node_2_fraud, sim_node_2_normal]).sample(frac=1, random_state=43).reset_index(drop=True)\n",
                "\n",
                "print(f\"Node 2 Fraud: {simulation_node_2['Class'].sum()}\")\n",
                "print(f\"Node 2 Normal: {len(simulation_node_2) - simulation_node_2['Class'].sum()}\")\n",
                "\n",
                "# Remove sampled data\n",
                "fraud_df = fraud_df.drop(sim_node_2_fraud.index)\n",
                "normal_df = normal_df.drop(sim_node_2_normal.index)\n",
                "\n",
                "# Save node2 simulation data\n",
                "simulation_node_2.to_csv(f'{data_dir}/simulation_node_2.csv', index=False)\n",
                "print(f'Simulation Node 2 saved: {simulation_node_2.shape}')\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# --- Train/Test Split ---\n",
                "remaining_df = pd.concat([fraud_df, normal_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
                "train_df, test_df = train_test_split(remaining_df, test_size=0.2, stratify=remaining_df['Class'], random_state=42)\n",
                "\n",
                "# Print train distributions\n",
                "print(f\"Train Fraud: {train_df['Class'].sum()}\")\n",
                "print(f\"Train Normal: {len(train_df) - train_df['Class'].sum()}\")\n",
                "\n",
                "# Save Train file\n",
                "train_df.to_csv(f'{data_dir}/train_data.csv', index=False)\n",
                "print(f'Train Data saved: {train_df.shape}')\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Print test distributions\n",
                "print(f\"Test Fraud: {test_df['Class'].sum()}\")\n",
                "print(f\"Test Normal: {len(test_df) - test_df['Class'].sum()}\")\n",
                "\n",
                "# Save Test file\n",
                "test_df.to_csv(f'{data_dir}/test_data.csv', index=False)\n",
                "print(f'Test Data saved: {test_df.shape}')\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Updated df for EDA\n",
                "df = remaining_df\n",
                "print(f'Updated main dataframe shape (Train + Test): {df.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda-header",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis (EDA) <a id='eda'></a>\n",
                "\n",
                "Comprehensive analysis of the dataset to understand patterns and characteristics."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda-basic-info",
            "metadata": {},
            "source": [
                "### 3.1 Basic Dataset Information"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "basic-info",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset overview\n",
                "print('Dataset Information:')\n",
                "print('=' * 50)\n",
                "print(f'Number of transactions: {len(df):,}')\n",
                "print(f'Number of features: {len(df.columns)}')\n",
                "print(f'\\nColumn names:\\n{df.columns.tolist()}')\n",
                "print(f'\\nData types:\\n{df.dtypes}')\n",
                "print(f'\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "data-quality",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values and duplicates\n",
                "print('Data Quality Check:')\n",
                "print('=' * 50)\n",
                "print(f'Missing values per column:\\n{df.isnull().sum()}')\n",
                "print(f'\\nTotal missing values: {df.isnull().sum().sum()}')\n",
                "print(f'\\nDuplicate rows: {df.duplicated().sum()}')\n",
                "print(f'\\nUnique classes: {df[\"Class\"].unique()}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "statistical-summary",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "print('Statistical Summary:')\n",
                "print('=' * 50)\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda-class-dist",
            "metadata": {},
            "source": [
                "### 3.2 Class Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "class-distribution",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze class distribution\n",
                "fraud_count = df['Class'].value_counts()\n",
                "fraud_percentage = df['Class'].value_counts(normalize=True) * 100\n",
                "\n",
                "print('Class Distribution:')\n",
                "print('=' * 50)\n",
                "print(f'Normal transactions (0): {fraud_count[0]:,} ({fraud_percentage[0]:.4f}%)')\n",
                "print(f'Fraudulent transactions (1): {fraud_count[1]:,} ({fraud_percentage[1]:.4f}%)')\n",
                "print(f'\\nFraud rate: {fraud_percentage[1]:.4f}%')\n",
                "print(f'Imbalance ratio: 1:{fraud_count[0]/fraud_count[1]:.0f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "plot-class-dist",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize class distribution\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Count plot\n",
                "sns.countplot(x='Class', data=df, ax=ax1, palette=['#3498db', '#e74c3c'])\n",
                "ax1.set_title('Fraud vs Normal Transaction Count', fontsize=14, fontweight='bold')\n",
                "ax1.set_xlabel('Class (0 = Normal, 1 = Fraud)', fontsize=12)\n",
                "ax1.set_ylabel('Count', fontsize=12)\n",
                "ax1.set_yscale('log')  # Log scale to see fraud cases better\n",
                "\n",
                "# Pie chart\n",
                "colors = ['#3498db', '#e74c3c']\n",
                "explode = (0, 0.1)\n",
                "ax2.pie(fraud_count, labels=['Normal', 'Fraud'], autopct='%1.4f%%', \n",
                "        colors=colors, explode=explode, shadow=True, startangle=90)\n",
                "ax2.set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eda-features",
            "metadata": {},
            "source": [
                "### 3.3 Feature Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "time-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Time feature analysis\n",
                "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
                "\n",
                "# Distribution of Time\n",
                "axes[0].hist(df['Time'], bins=50, color='#3498db', edgecolor='black')\n",
                "axes[0].set_title('Distribution of Transaction Times', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Time (seconds)', fontsize=12)\n",
                "axes[0].set_ylabel('Frequency', fontsize=12)\n",
                "\n",
                "# Time distribution by class\n",
                "df[df['Class'] == 0]['Time'].hist(bins=50, alpha=0.5, label='Normal', ax=axes[1], color='#3498db')\n",
                "df[df['Class'] == 1]['Time'].hist(bins=50, alpha=0.5, label='Fraud', ax=axes[1], color='#e74c3c')\n",
                "axes[1].set_title('Transaction Time Distribution by Class', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Time (seconds)', fontsize=12)\n",
                "axes[1].set_ylabel('Frequency', fontsize=12)\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "amount-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Amount feature analysis\n",
                "print('Amount Statistics by Class:')\n",
                "print('=' * 50)\n",
                "print('Normal transactions:')\n",
                "print(df[df['Class'] == 0]['Amount'].describe())\n",
                "print('\\nFraudulent transactions:')\n",
                "print(df[df['Class'] == 1]['Amount'].describe())\n",
                "\n",
                "# Visualize amount distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Box plot\n",
                "sns.boxplot(x='Class', y='Amount', data=df, ax=axes[0], palette=['#3498db', '#e74c3c'])\n",
                "axes[0].set_title('Amount Distribution by Class', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Class (0 = Normal, 1 = Fraud)', fontsize=12)\n",
                "axes[0].set_ylabel('Transaction Amount', fontsize=12)\n",
                "\n",
                "# Histogram\n",
                "df[df['Class'] == 0]['Amount'].hist(bins=50, alpha=0.5, label='Normal', ax=axes[1], color='#3498db')\n",
                "df[df['Class'] == 1]['Amount'].hist(bins=50, alpha=0.5, label='Fraud', ax=axes[1], color='#e74c3c')\n",
                "axes[1].set_title('Amount Distribution Comparison', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Amount', fontsize=12)\n",
                "axes[1].set_ylabel('Frequency', fontsize=12)\n",
                "axes[1].set_xlim([0, 500])  # Limit for better visibility\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "correlation-analysis",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation analysis for key features\n",
                "# Select a subset of features for correlation (V1-V10 + Amount + Class)\n",
                "corr_features = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'Amount', 'Class']\n",
                "correlation_matrix = df[corr_features].corr()\n",
                "\n",
                "# Plot correlation heatmap\n",
                "plt.figure(figsize=(12, 10))\n",
                "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
                "            center=0, square=True, linewidths=1)\n",
                "plt.title('Correlation Matrix (Selected Features)', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training-header",
            "metadata": {},
            "source": [
                "## 4. Model Training <a id='training'></a>\n",
                "\n",
                "Training and evaluating machine learning models for fraud detection."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data-prep",
            "metadata": {},
            "source": [
                "### 4.1 Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "prepare-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the pre-split training and testing data\n",
                "train_df = pd.read_csv(f'{data_dir}/train_data.csv')\n",
                "test_df = pd.read_csv(f'{data_dir}/test_data.csv')\n",
                "\n",
                "# Separate features and target\n",
                "X_train = train_df.drop('Class', axis=1)\n",
                "y_train = train_df['Class']\n",
                "X_test = test_df.drop('Class', axis=1)\n",
                "y_test = test_df['Class']\n",
                "\n",
                "print('Data Preparation (Loaded from Split Files):')\n",
                "print('=' * 50)\n",
                "print(f'Training set: {X_train.shape[0]:,} samples')\n",
                "print(f'Testing set: {X_test.shape[0]:,} samples')\n",
                "print(f'\\nClass distribution in training set:')\n",
                "print(y_train.value_counts())\n",
                "print(f'\\nClass distribution in testing set:')\n",
                "print(y_test.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "scale-data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature scaling\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print('Features scaled successfully using StandardScaler')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "handle-imbalance",
            "metadata": {},
            "source": [
                "### 4.2 Handling Class Imbalance with SMOTE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "smote",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply SMOTE to balance the dataset\n",
                "print('Applying SMOTE...')\n",
                "smote = SMOTE(random_state=42)\n",
                "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
                "\n",
                "print('\\nClass distribution after SMOTE:')\n",
                "print('=' * 50)\n",
                "print(pd.Series(y_train_balanced).value_counts())\n",
                "print(f'\\nBalanced training set size: {len(X_train_balanced):,}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model-training",
            "metadata": {},
            "source": [
                "### 4.3 Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "logistic-regression",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Logistic Regression\n",
                "print('Training Logistic Regression...')\n",
                "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
                "lr_model.fit(X_train_balanced, y_train_balanced)\n",
                "print('Logistic Regression trained')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "random-forest",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Random Forest\n",
                "print('Training Random Forest...')\n",
                "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
                "rf_model.fit(X_train_balanced, y_train_balanced)\n",
                "print('Random Forest trained')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "xgboost",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train XGBoost\n",
                "print('Training XGBoost...')\n",
                "xgb_model = xgb.XGBClassifier(\n",
                "    n_estimators=100,\n",
                "    random_state=42,\n",
                "    use_label_encoder=False,\n",
                "    eval_metric='logloss'\n",
                ")\n",
                "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
                "print('XGBoost trained')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model-evaluation",
            "metadata": {},
            "source": [
                "### 4.4 Model Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eval-function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to evaluate model performance\n",
                "from sklearn.metrics import fbeta_score\n",
                "\n",
                "def evaluate_model(model, model_name, X_test, y_test):\n",
                "    \"\"\"Evaluate model and print metrics\"\"\"\n",
                "    print(f'\\n{model_name} Performance:')\n",
                "    print('=' * 60)\n",
                "    \n",
                "    # Make predictions\n",
                "    y_pred = model.predict(X_test)\n",
                "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
                "    \n",
                "    # Calculate metrics\n",
                "    accuracy = accuracy_score(y_test, y_pred)\n",
                "    precision = precision_score(y_test, y_pred)\n",
                "    recall = recall_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred)\n",
                "    f2 = fbeta_score(y_test, y_pred, beta=2) # F2 Score favors recall\n",
                "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
                "    \n",
                "    # Print metrics\n",
                "    print(f'Accuracy:  {accuracy:.4f}')\n",
                "    print(f'Precision: {precision:.4f}')\n",
                "    print(f'Recall:    {recall:.4f}')\n",
                "    print(f'F1-Score:  {f1:.4f}')\n",
                "    print(f'F2-Score:  {f2:.4f}')\n",
                "    print(f'ROC-AUC:   {roc_auc:.4f}')\n",
                "    \n",
                "    # Confusion Matrix\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    print(f'\\nConfusion Matrix:')\n",
                "    print(cm)\n",
                "    \n",
                "    # Classification Report\n",
                "    print(f'\\nClassification Report:')\n",
                "    print(classification_report(y_test, y_pred, target_names=['Normal', 'Fraud']))\n",
                "    \n",
                "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, \n",
                "            'f1': f1, 'f2': f2, 'roc_auc': roc_auc, 'cm': cm, \n",
                "            'y_pred': y_pred, 'y_pred_proba': y_pred_proba}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "evaluate-models",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate all models\n",
                "lr_results = evaluate_model(lr_model, 'Logistic Regression', X_test_scaled, y_test)\n",
                "rf_results = evaluate_model(rf_model, 'Random Forest', X_test_scaled, y_test)\n",
                "xgb_results = evaluate_model(xgb_model, 'XGBoost', X_test_scaled, y_test)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "compare-models",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare models\n",
                "comparison_df = pd.DataFrame({\n",
                "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
                "    'Accuracy': [lr_results['accuracy'], rf_results['accuracy'], xgb_results['accuracy']],\n",
                "    'Precision': [lr_results['precision'], rf_results['precision'], xgb_results['precision']],\n",
                "    'Recall': [lr_results['recall'], rf_results['recall'], xgb_results['recall']],\n",
                "    'F1-Score': [lr_results['f1'], rf_results['f1'], xgb_results['f1']],\n",
                "    'F2-Score': [lr_results['f2'], rf_results['f2'], xgb_results['f2']],\n",
                "    'ROC-AUC': [lr_results['roc_auc'], rf_results['roc_auc'], xgb_results['roc_auc']]\n",
                "})\n",
                "\n",
                "print('\\nModel Comparison:')\n",
                "print('=' * 80)\n",
                "print(comparison_df.to_string(index=False))\n",
                "\n",
                "# Plot comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Bar plot for all metrics\n",
                "comparison_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'F2-Score', 'ROC-AUC']].plot(\n",
                "    kind='bar', ax=axes[0], rot=45\n",
                ")\n",
                "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylabel('Score', fontsize=12)\n",
                "axes[0].legend(loc='lower right')\n",
                "axes[0].set_ylim([0.8, 1.0])\n",
                "\n",
                "# ROC Curves\n",
                "for name, results in [('Logistic Regression', lr_results), \n",
                "                       ('Random Forest', rf_results), \n",
                "                       ('XGBoost', xgb_results)]:\n",
                "    fpr, tpr, _ = roc_curve(y_test, results['y_pred_proba'])\n",
                "    axes[1].plot(fpr, tpr, label=f'{name} (AUC = {results[\"roc_auc\"]:.4f})')\n",
                "\n",
                "axes[1].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
                "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
                "axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
                "axes[1].set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
                "axes[1].legend()\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "confusion-matrices",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot confusion matrices\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "for idx, (name, results) in enumerate([('Logistic Regression', lr_results), \n",
                "                                        ('Random Forest', rf_results), \n",
                "                                        ('XGBoost', xgb_results)]):\n",
                "    sns.heatmap(results['cm'], annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
                "    axes[idx].set_title(f'{name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
                "    axes[idx].set_ylabel('True Label', fontsize=10)\n",
                "    axes[idx].set_xlabel('Predicted Label', fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save-models",
            "metadata": {},
            "source": [
                "### 4.5 Save Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save-best-model",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Determine best model based on F2-Score (optimizing for Recall and F1)\n",
                "# F2 score weights recall higher than precision, which is crucial for fraud detection\n",
                "best_model_idx = comparison_df['F2-Score'].idxmax()\n",
                "best_model_name = comparison_df.loc[best_model_idx, 'Model']\n",
                "best_model = [lr_model, rf_model, xgb_model][best_model_idx]\n",
                "\n",
                "print(f'Best Model: {best_model_name}')\n",
                "print(f'F2-Score: {comparison_df.loc[best_model_idx, \"F2-Score\"]:.4f}')\n",
                "print(f'Recall: {comparison_df.loc[best_model_idx, \"Recall\"]:.4f}')\n",
                "print(f'F1-Score: {comparison_df.loc[best_model_idx, \"F1-Score\"]:.4f}')\n",
                "\n",
                "# Save the best model and scaler\n",
                "model_dir = '../models'\n",
                "if not os.path.exists(model_dir):\n",
                "    os.makedirs(model_dir)\n",
                "    print(f'Created directory: {model_dir}')\n",
                "\n",
                "best_model_name = best_model_name.replace(' ', '_').lower()\n",
                "joblib.dump(best_model, f'{model_dir}/best_model_{best_model_name}.pkl')\n",
                "joblib.dump(scaler, f'{model_dir}/scaler.pkl')\n",
                "\n",
                "print(f'\\nBest model saved as: {model_dir}/best_model_{best_model_name}.pkl')\n",
                "print(f'Scaler saved as: {model_dir}/scaler.pkl')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
